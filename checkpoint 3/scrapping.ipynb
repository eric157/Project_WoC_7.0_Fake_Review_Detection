{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jLoI9-PmvrCX"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ch0gxmksvrCZ"
      },
      "outputs": [],
      "source": [
        "def is_amazon_url(url):\n",
        "    \"\"\"Checks if a URL is a valid amazon.com product URL.\"\"\"\n",
        "    amazon_pattern = r\"(https?://)?(www.)?amazon\\.com/.*\"\n",
        "    return bool(re.match(amazon_pattern, url))\n",
        "\n",
        "def save_reviews_to_csv(reviews, filename='scraped_reviews.csv'):\n",
        "    \"\"\"Saves a list of reviews to a CSV file.\"\"\"\n",
        "    if not reviews:\n",
        "        logging.info(\"No reviews to save.\")\n",
        "        return\n",
        "    \n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"text\", \"rating\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(reviews)\n",
        "    logging.info(f\"Saved {len(reviews)} reviews to '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In0nh8JjvrCa",
        "outputId": "6622de03-7e17-456a-9f8c-e6d22b317429"
      },
      "outputs": [],
      "source": [
        "def scrape_amazon_reviews(product_url, max_reviews=50, phone_number=None, password=None):\n",
        "    \"\"\"\n",
        "    Scrapes reviews from an Amazon product page, handling dynamic loading, pagination, and CAPTCHAs.\n",
        "    Attempts to load cookies first, if available, to bypass login.\n",
        "\n",
        "    Args:\n",
        "        product_url (str): The URL of the Amazon product page.\n",
        "        max_reviews (int, optional): Maximum number of reviews to scrape. Defaults to 50.\n",
        "        phone_number (str, optional): Amazon account phone number.\n",
        "        password (str, optional): Amazon account password.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing review text and rating.\n",
        "            Returns an empty list if no reviews were scraped or login/captcha/cookie load fails.\n",
        "    \"\"\"\n",
        "\n",
        "    if not is_amazon_url(product_url):\n",
        "        raise ValueError(\"Invalid amazon.com product URL.\")\n",
        "\n",
        "    scraped_reviews = []\n",
        "    review_count = 0\n",
        "    page_number = 1\n",
        "    cookies_file_path = \"amazon_cookies.pkl\"\n",
        "    \n",
        "    # Set up Chrome options for headless mode\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver_headless = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Navigating to product page in headless mode.\")\n",
        "        driver_headless.get(product_url)\n",
        "        \n",
        "        try:\n",
        "            logging.info(\"Loading cookies.\")\n",
        "            cookies = pickle.load(open(cookies_file_path, \"rb\"))\n",
        "            for cookie in cookies:\n",
        "                driver_headless.add_cookie(cookie)\n",
        "            driver_headless.get(product_url)\n",
        "            logging.info(\"Cookies loaded successfully. Bypassing login.\")\n",
        "        except Exception as cookie_e:\n",
        "            logging.warning(f\"Error loading cookies, proceeding with login. {cookie_e}\")\n",
        "            \n",
        "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
        "            try:\n",
        "                logging.info(\"Attempting to log in.\")\n",
        "                driver.get(product_url)\n",
        "\n",
        "                login_link = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#nav-link-accountList\"))\n",
        "                )\n",
        "                login_link.click()\n",
        "                time.sleep(2)\n",
        "\n",
        "                phone_input = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#ap_email\"))\n",
        "                )\n",
        "                phone_input.send_keys(phone_number)\n",
        "\n",
        "                continue_button = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#continue\"))\n",
        "                )\n",
        "                continue_button.click()\n",
        "                time.sleep(2)\n",
        "\n",
        "                password_input = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#ap_password\"))\n",
        "                )\n",
        "                password_input.send_keys(password)\n",
        "\n",
        "                sign_in_button = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#signInSubmit\"))\n",
        "                )\n",
        "                sign_in_button.click()\n",
        "                time.sleep(5)\n",
        "\n",
        "                if driver.find_elements(By.CSS_SELECTOR, '#auth-captcha-guess-text'):\n",
        "                    logging.error(\"Captcha detected. Please solve it manually.\")\n",
        "                    logging.info(\"Sleeping for 2 minutes to solve captcha.\")\n",
        "                    time.sleep(120)  # Giving 2 minutes for captcha solving\n",
        "                    if driver.find_elements(By.CSS_SELECTOR, '#auth-captcha-guess-text'):\n",
        "                        logging.error(\"Captcha was not solved. Login failed\")\n",
        "                        driver.quit()\n",
        "                        driver_headless.quit()\n",
        "                        return []\n",
        "                else:\n",
        "                    logging.info(\"Captcha was not detected.\")\n",
        "                logging.info(\"Login successful\")\n",
        "                \n",
        "                logging.info(\"Saving cookies after successful login.\")\n",
        "                pickle.dump(driver.get_cookies(), open(cookies_file_path, \"wb\"))\n",
        "                \n",
        "            except Exception as login_e:\n",
        "                logging.error(f\"Login failed: {login_e}\")\n",
        "                driver.quit()\n",
        "                driver_headless.quit()\n",
        "                return []\n",
        "            finally:\n",
        "                driver.quit()\n",
        "                logging.info(\"Driver quitted after solving captcha.\")\n",
        "            \n",
        "        try:\n",
        "            logging.info(\"Attempting to find 'see all reviews' link\")\n",
        "            see_all_link = WebDriverWait(driver_headless, 20).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[data-hook='see-all-reviews-link-foot']\"))\n",
        "             )\n",
        "            \n",
        "            logging.info(\"Found 'see all reviews' link. Clicking it.\")\n",
        "            see_all_link.click()\n",
        "            time.sleep(5)\n",
        "            \n",
        "        except Exception as e:\n",
        "             logging.error(f\"Could not find 'see all reviews' link {e}\")\n",
        "             driver_headless.quit()\n",
        "             return scraped_reviews\n",
        "        \n",
        "        logging.info(\"Navigated to review page\")\n",
        "        \n",
        "        while review_count < max_reviews:\n",
        "          logging.info(f\"Fetching reviews from page: {page_number}\")\n",
        "          \n",
        "          \n",
        "          try:\n",
        "            WebDriverWait(driver_headless, 20).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, '[data-hook=\"review\"]'))\n",
        "             )\n",
        "          except:\n",
        "             logging.info(\"Could not find any reviews in the page.\")\n",
        "             break\n",
        "          \n",
        "          soup = BeautifulSoup(driver_headless.page_source, 'html.parser')\n",
        "          review_elements = soup.select('[data-hook=\"review\"]')\n",
        "          \n",
        "          if not review_elements:\n",
        "            logging.info(f\"No reviews found on page {page_number}\")\n",
        "            break\n",
        "\n",
        "          for review in review_elements:\n",
        "            try:\n",
        "              text_element = review.select_one('[data-hook=\"review-body\"]')\n",
        "              rating_element = review.select_one('[data-hook=\"review-star-rating\"] > span.a-icon-alt')\n",
        "              \n",
        "              if text_element and rating_element:\n",
        "                text = text_element.get_text(strip=True)\n",
        "                rating_text = rating_element.get_text(strip=True)\n",
        "                \n",
        "                # Extract the numerical rating using regex\n",
        "                rating_match = re.search(r'(\\d+(\\.\\d+)?)', rating_text)\n",
        "                rating = rating_match.group(1) if rating_match else None\n",
        "\n",
        "                scraped_reviews.append({\n",
        "                    \"text\": text,\n",
        "                    \"rating\": rating,\n",
        "                    })\n",
        "                review_count += 1\n",
        "                if review_count >= max_reviews:\n",
        "                  break\n",
        "              else:\n",
        "                logging.warning(f\"Skipping review due to missing text or rating\")\n",
        "            except Exception as inner_e:\n",
        "               logging.error(f\"Error parsing review: {inner_e}\")\n",
        "          if review_count >= max_reviews:\n",
        "            break\n",
        "          \n",
        "          try:\n",
        "              next_button = WebDriverWait(driver_headless, 20).until(\n",
        "                  EC.presence_of_element_located((By.CSS_SELECTOR, \"li.a-last > a\"))\n",
        "              )\n",
        "              if next_button:\n",
        "                  next_button.click()\n",
        "                  page_number+=1\n",
        "                  time.sleep(5)\n",
        "                  logging.info(\"Moving to next page\")\n",
        "              else:\n",
        "                  logging.info(\"Next page button not found. Stopping pagination.\")\n",
        "                  break\n",
        "          except Exception as next_e:\n",
        "                logging.info(f\"Next page button not found: {next_e}. Stopping pagination.\")\n",
        "                break\n",
        "            \n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during scraping: {e}\")\n",
        "    finally:\n",
        "        driver_headless.quit()\n",
        "        logging.info(\"Driver quitted after reviews scraping.\")\n",
        "\n",
        "    return scraped_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oG3iaNwNZ9-f",
        "outputId": "b04cfc52-a0e6-4a86-b9e9-f9dcb8aecd37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-28 01:30:38,206 - INFO - ====== WebDriver manager ======\n",
            "2025-01-28 01:30:39,564 - INFO - Get LATEST chromedriver version for google-chrome\n",
            "2025-01-28 01:30:39,693 - INFO - Get LATEST chromedriver version for google-chrome\n",
            "2025-01-28 01:30:39,809 - INFO - Driver [C:\\Users\\ericp\\.wdm\\drivers\\chromedriver\\win64\\131.0.6778.264\\chromedriver-win32/chromedriver.exe] found in cache\n",
            "2025-01-28 01:30:41,431 - INFO - Navigating to product page in headless mode.\n",
            "2025-01-28 01:30:44,236 - INFO - Loading cookies.\n",
            "2025-01-28 01:30:47,071 - INFO - Cookies loaded successfully. Bypassing login.\n",
            "2025-01-28 01:30:47,071 - INFO - Attempting to find 'see all reviews' link\n",
            "2025-01-28 01:30:47,142 - INFO - Found 'see all reviews' link. Clicking it.\n",
            "2025-01-28 01:30:53,135 - INFO - Navigated to review page\n",
            "2025-01-28 01:30:53,135 - INFO - Fetching reviews from page: 1\n",
            "2025-01-28 01:30:58,394 - INFO - Moving to next page\n",
            "2025-01-28 01:30:58,394 - INFO - Fetching reviews from page: 2\n",
            "2025-01-28 01:31:03,641 - INFO - Moving to next page\n",
            "2025-01-28 01:31:03,641 - INFO - Fetching reviews from page: 3\n",
            "2025-01-28 01:31:08,852 - INFO - Moving to next page\n",
            "2025-01-28 01:31:08,853 - INFO - Fetching reviews from page: 4\n",
            "2025-01-28 01:31:14,063 - INFO - Moving to next page\n",
            "2025-01-28 01:31:14,064 - INFO - Fetching reviews from page: 5\n",
            "2025-01-28 01:31:19,280 - INFO - Moving to next page\n",
            "2025-01-28 01:31:19,280 - INFO - Fetching reviews from page: 6\n",
            "2025-01-28 01:31:24,485 - INFO - Moving to next page\n",
            "2025-01-28 01:31:24,486 - INFO - Fetching reviews from page: 7\n",
            "2025-01-28 01:31:29,739 - INFO - Moving to next page\n",
            "2025-01-28 01:31:29,739 - INFO - Fetching reviews from page: 8\n",
            "2025-01-28 01:31:34,967 - INFO - Moving to next page\n",
            "2025-01-28 01:31:34,968 - INFO - Fetching reviews from page: 9\n",
            "2025-01-28 01:31:40,188 - INFO - Moving to next page\n",
            "2025-01-28 01:31:40,189 - INFO - Fetching reviews from page: 10\n",
            "2025-01-28 01:31:42,506 - INFO - Driver quitted after reviews scraping.\n",
            "2025-01-28 01:31:42,509 - INFO - Saved 100 reviews to 'scraped_reviews.csv'.\n",
            "2025-01-28 01:31:42,510 - INFO - Scraping Complete.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        product_url = \"https://www.amazon.com/Apple-iPhone-XR-128GB-Black/dp/B087YB8DXC/ref=sr_1_8?sr=8-8https://www.amazon.com/Apple-iPhone-XR-128GB-Black/dp/B087YB8DXC/ref=sr_1_8?sr=8-8\"\n",
        "        phone_number = \"9727715703\"\n",
        "        password = \"kuku@1108\"\n",
        "        reviews = scrape_amazon_reviews(product_url, max_reviews=100, phone_number=phone_number, password=password)\n",
        "        save_reviews_to_csv(reviews)\n",
        "        if reviews:\n",
        "            logging.info(\"Scraping Complete.\")\n",
        "        else:\n",
        "            logging.info(\"No reviews were scraped.\")\n",
        "    except ValueError as e:\n",
        "         logging.error(e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
